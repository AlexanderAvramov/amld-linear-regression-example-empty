{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee62f01",
   "metadata": {},
   "source": [
    "# Welcome to the Linear Regression Example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda79260",
   "metadata": {},
   "source": [
    "## Workshop Steps\n",
    "Now that you have opened up the MyBinder environment and are reading this, you are already on the right track! Inside this environment,\n",
    "you will also find:\n",
    "* sample scripts: This is a folder containing the base of the scripts that you will be working with to finish the exercise. Please look for the triple exclamation points (!!!) as that means that you are being asked to write some code to get things to work!\n",
    "* README.md: This is just the README file you saw on the Github page.\n",
    "* requirements.txt: This is a list of the required libraries that were installed upon startup.\n",
    "* setup.ipynb: The file you are reading right now! Think of this as your home page.\n",
    "* VW ID. 3 Pro Max EV Consumption.csv: The raw .csv file from Kaggle.com. Please note that we will need to move this into the data job's folder once we create it for a neater environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a7011",
   "metadata": {},
   "source": [
    "### Step 1: Explore VDK's Functionalities\n",
    "A simple command like that found in the setup.ipynb \"!vdk --help\" gives you all the information you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82213d",
   "metadata": {},
   "source": [
    "### Step 2: Create a Data Job\n",
    "Now that we have explored VDK's capabilities, let's create our data job. \n",
    "\n",
    "Keep in mind that we would like to have a sub-folder for the data job,so that our Streamlit script is outside of it and in the main directory. \n",
    "\n",
    "Based on the information above, try creating a data job titled \"linear-reg-data-job\". You can chose any team name that you want, but please create the job at the home directory. This will create a sub-folder for the data job. The home directory is /home/jovyan. \n",
    "\n",
    "Here's an example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31519980",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk create -n linear-reg-data-job -t team-awesome -p /home/jovyan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfd927",
   "metadata": {},
   "source": [
    "### Step 3: Work Out the Data Job Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994dc06",
   "metadata": {},
   "source": [
    "Now that you have created a data job, please go inside the subfolder and set up the structure of your data job. Here's the general idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8d516",
   "metadata": {},
   "source": [
    "We want the data job to have four scripts:\n",
    "* Let's have one Python script that reads in the data and strips its special characters and re-saves it.\n",
    "* Let's have another Python script that reads in the fixed data and performs exploratory data analysis.\n",
    "* Let's have a third Python script that reads in the data from the first script, cleans up the data, and gets it ready for model building and testing.\n",
    "* Lastly, let's have a Python script that reads the data from the third script, builds a simple Linear Regression model, tests it, and saves it.\n",
    "\n",
    "Each of these four scripts are present in the sample scripts subfolder. However, we've added some coding challeneges inside of them to make things fun! Let's move those four scripts to the data job subfolder. Please run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daeb899",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv \"sample scripts/10_read_in_data.py\" ~/linear-reg-data-job\n",
    "! mv \"sample scripts/20_explore_data.py\" ~/linear-reg-data-job\n",
    "! mv \"sample scripts/30_process_data.py\" ~/linear-reg-data-job\n",
    "! mv \"sample scripts/40_build_model.py\"  ~/linear-reg-data-job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac987c",
   "metadata": {},
   "source": [
    "Let's move the raw CSV file to the data job's subfolder. It's not usually necessary, but it will create a sense of a neater working environment here. As such, please execute the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv \"VW ID. 3 Pro Max EV Consumption.csv\" ~/linear-reg-data-job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b5789",
   "metadata": {},
   "source": [
    "Let's also delete the other template files that we will not be needing:\n",
    "* the SQL script: our example does not do anything with SQL\n",
    "* the sample Python script: we already have moved four sample Python scripts, so we won't be needing this\n",
    "* README.md: We already have a README for the entire example, so we can get rid of this\n",
    "* requirements.txt: Each data job would need this file if the data job relies on external libraries that VDK does not have. In our case, MyBinder installed those upon startup, so we won't be needing this either.\n",
    "\n",
    "As such, please run the code below to delete them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm \"linear-reg-data-job/10_sql_step.sql\"\n",
    "! rm \"linear-reg-data-job/20_python_step.py\"\n",
    "! rm \"linear-reg-data-job/README.md\"\n",
    "! rm \"linear-reg-data-job/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1d2c9",
   "metadata": {},
   "source": [
    "Great! Now you're all set up with the data job:\n",
    "* You have created a data job.\n",
    "* You have deleted the template files that you do not need.\n",
    "* You have moved the sample scripts we provided to the data job sub-folder.\n",
    "* You have moved the raw CSV file to the data job sub-folder for a neater environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0d0f3",
   "metadata": {},
   "source": [
    "The next step is to begin working on each script in the data job! Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47ca4c",
   "metadata": {},
   "source": [
    "### Step 4: Data Job - Read in the Data and Strip Special Characters (10_read_in_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40088e",
   "metadata": {},
   "source": [
    "Please open up 10_read_in_data.py. Inside it, you will see the code template already populated. Let's explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d19615",
   "metadata": {},
   "source": [
    "First, we import all of the necessary libraries.\n",
    "\n",
    "Then, we initiallize a logging variable and change the directory to the script's location.\n",
    "\n",
    "Then, we open up VDK's \"run\" function. This is how VDK knows that the following code will be part of its execution path, if you will."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477f224",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed93e23",
   "metadata": {},
   "source": [
    "Once we request that the log print out the file name's execution status (line 11), we want YOU to make an edit! Please see line 14. Here, we want you to enter the filename of the raw CSV file, so that it is stored in the 'filename_to_import' variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36083e40",
   "metadata": {},
   "source": [
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e584f7",
   "metadata": {},
   "source": [
    "Now that you have done this, we read in the data using Pandas' read_csv functionality and use an encoding to allow for the special characters. However, we will later want to strip them from the column names. Lines 25-29 do that by first:\n",
    "* Getting rid of non-alphanumeric characters and replacing them with blanks.\n",
    "* Stripping any leading or trailing whitespace.\n",
    "* Replacing any spacing in between characters with an underscore.\n",
    "* Making all of the column names in lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc302f0",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66868116",
   "metadata": {},
   "source": [
    "Please note that line 29 has a bit for you to do! Please enter the appropriate method to turn the column names to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed803c7",
   "metadata": {},
   "source": [
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9f263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32fa17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65735d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2a8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8acac6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a673786e",
   "metadata": {},
   "source": [
    "### Step 5: Data Job - Exploratory Data Analysis\n",
    "Let's go back to the linear-reg-data-job sub-folder and make a copy of the 10_read_in_data.py file. Let's call the copy 20_explore_data.py.\n",
    "Open up the copy and let's make a few changes. \n",
    "\n",
    "Because we will be making some charts and tables, we will need to import some of the libraries that we installed from the requirements.txt file earlier.\n",
    "As such, please delete all the code before the run function and paste the following:\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "import pathlib\n",
    "from vdk.api.job_input import IJobInput\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "os.chdir(pathlib.Path(__file__).parent.absolute())\n",
    "```\n",
    "\n",
    "Now, inside the run function, let's delete everything. We will write code that creates a sub-folder within the linear-reg-data-job sub-folder, which\n",
    "will store all the exploratory graphics and tables. As such, delete everything and paste this:\n",
    "```\n",
    "logger.info('executing program: ' + os.path.basename(__file__))\n",
    "\n",
    "if not os.path.exists('explore_data'):\n",
    "    os.makedirs('explore_data')\n",
    "```\n",
    "\n",
    "Let's now read in the fixed .CSV file. Please paste the following code in the run function below the code that created the explore_data sub-folder.\n",
    "Please note, however, that we've left the filename_to_import variable [BLANK]. Please change that to the name of the fixed .CSV file.\n",
    "```\n",
    "filename_to_import = [BLANK]\n",
    "df = pd.read_csv(filepath_or_buffer=filename_to_import, parse_dates=['date'])\n",
    "```\n",
    "\n",
    "Let's also write some code below that to explore the data. Make sure to write it to the log using\n",
    "```\n",
    "logger.info(df.info())\n",
    "```\n",
    "as an example. Try logging the head and the tail of the data. Get creative with it!\n",
    "\n",
    "At some point, however, we will need to define the numeric variables. Let's write the following code to store the column names of the numeric\n",
    "variables.\n",
    "```\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "```\n",
    "\n",
    "Ok, now that we have the numeric columns. Let's create some histograms for each numeric column. Here is one way to do it using seaborn and matplotlib.\n",
    "Try out some different functions. Try changing distplot to something else.\n",
    "```\n",
    "sns.set(\n",
    "    style='ticks',\n",
    "    rc={\n",
    "        'axes.spines.right': False,\n",
    "        'axes.spines.top': False,\n",
    "        'figure.figsize': (18, 14)\n",
    "        }\n",
    "    )\n",
    "for num_col in num_cols:\n",
    "    sns.distplot(df[num_col],\n",
    "        bins=100).set(\n",
    "            xlabel=num_col,\n",
    "            ylabel='Count'\n",
    "            )\n",
    "    plt.savefig('explore_data/' + num_col + '.png')\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "```\n",
    "\n",
    "Ok, now that we have created a histogram for each numeric variable and saved that histogram in the explore_data subfolder, let's turn our attention\n",
    "to the categorical variables. Let's follow the same process as that above. In other words, let's first create a list of the names of the columns\n",
    "that are categorical! We've left a little challenge for you in it: change [BLANK] to something that works!\n",
    "```\n",
    "cat_cols = [i for i in df.columns if i not in [BLANK]]\n",
    "```\n",
    "\n",
    "Let's now run a loop that calculates the value counts for each categorical variable. What does that mean? We want to see the values that occur\n",
    "for each categorical variable and how often those values occur. We also want to save the result in an Excel file, where each worksheet within the\n",
    "Excel file is a value count for each categorical variable. Here's the code to do that:\n",
    "\n",
    "```\n",
    "cat_writer = pd.ExcelWriter('explore_data/explore_categoricals.xlsx', engine='xlsxwriter')\n",
    "for cat_col in cat_cols:\n",
    "    temp = pd.DataFrame(\n",
    "        df[cat_col].value_counts(dropna=False)\n",
    "    )\n",
    "    temp.to_excel(cat_writer, sheet_name=cat_col)\n",
    "cat_writer.save()\n",
    "```\n",
    "\n",
    "After all this, let's go and run the data job again by re-running the \"!vdk run...\" command in the setup.ipynb.\n",
    "This time, notice that it will run both scripts, one after the other in an alphabetical order. Let's go and check out the \n",
    "results! Please head over to the explore_data sub-folder within the linear-reg-data-job folder!\n",
    "Congrats!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb999c8",
   "metadata": {},
   "source": [
    "### Step 6: Data Job - Processing The Data\n",
    "Now that we have explored the data, we know what we want to do and what we must do. Let's head over to the linear-reg-data-job sub-folder and create\n",
    "a copy of the last Python script. Rename it 30_process_data.py.\n",
    "\n",
    "Let's delete everything above the run function, import our libraries and initialize the log:\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import pathlib\n",
    "import os\n",
    "from vdk.api.job_input import IJobInput\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "os.chdir(pathlib.Path(__file__).parent.absolute())\n",
    "```\n",
    "\n",
    "Now, inside the run function, delete everything. Let's define the data sets that we will be reading in and later\n",
    "saving and read in the data:\n",
    "```\n",
    "logger.info('executing program: ' + os.path.basename(__file__))\n",
    "\n",
    "# some definitions...\n",
    "filename_to_import = 'VW ID. 3 Pro Max EV Consumption_Fixed_Columns.csv'\n",
    "filename_to_export = 'VW ID. 3 Pro Max EV Consumption_Model_Data.csv'\n",
    "\n",
    "# reading in the data...\n",
    "df = pd.read_csv(filepath_or_buffer=filename_to_import, parse_dates=['date'])\n",
    "```\n",
    "\n",
    "If you remember from the exploratory data analysis section, we had some missing values in our data set. There many ways to deal with missing values\n",
    "but the simplest is to just drop observations where any column has a missing value. In our case, that doesn't drop too many data points, so let's go\n",
    "ahead and do that:\n",
    "```\n",
    "df_no_nulls = df.copy().dropna()\n",
    "```\n",
    "\n",
    "Linear regression only works with numeric variables. As such, if we want to use the categorical variables, we will need to turn them into numerics:\n",
    "```\n",
    "df_no_nulls['ac_use'] = np.where(df_no_nulls['ac_c'] == 'OFF', 0, 1)\n",
    "\n",
    "df_no_nulls['heated_seats'] = np.where(df_no_nulls['heated_front_seats_level'] == 0, 0, 1)\n",
    "\n",
    "df_no_nulls['eco_mode'] = np.where(df_no_nulls['mode'] == 'ECO', 1, 0)\n",
    "\n",
    "df_no_nulls[\"is_summer\"] = np.where((df_no_nulls['date'] >= '2021-06-21') & (df_no_nulls['date'] <= '2021-09-22'), 1, 0)\n",
    "\n",
    "df_no_nulls['is_bridgestone_tyre'] = np.where(df_no_nulls['tyres'] == 'Bridgestone 215/45 R20 LM32', 1, 0)\n",
    "```\n",
    "\n",
    "Let's now create our dependent variable - the variable we will be trying to estimate: battery drainage:\n",
    "```\n",
    "df_no_nulls['battery_drain'] = -(df_no_nulls['charge_level_end'] - df_no_nulls['charge_level_start'])\n",
    "```\n",
    "\n",
    "Lastly, let's also create one more variable: temperature change. Who knows - it may have explanatory power!\n",
    "```\n",
    "df_no_nulls['temperature_increase'] = df_no_nulls['temperature_end_c'] - df_no_nulls['temperature_start_c']\n",
    "```\n",
    "\n",
    "Let's now limit the data set to the variables we want. This helps declutter.\n",
    "```\n",
    "df_no_nulls_limited = df_no_nulls.copy()[\n",
    "    [\n",
    "        'battery_drain',\n",
    "        'charge_level_start',\n",
    "        'is_bridgestone_tyre',\n",
    "        'temperature_start_c',\n",
    "        'temperature_increase',\n",
    "        'distance_km',\n",
    "        'average_speed_kmh',\n",
    "        'average_consumption_kwhkm',\n",
    "        'ac_use',\n",
    "        'heated_seats',\n",
    "        'eco_mode',\n",
    "        'is_summer'\n",
    "    ]\n",
    "]\n",
    "```\n",
    "\n",
    "It is always good practice, however, to look at the variables you created. Who knows - maybe you didn't see something with regard to the relationship\n",
    "between some of the variables. For example, temperature start and temperature end might look perfectly fine on their own, but if we calculate the change\n",
    "we may find some data entry error if we see that the temperature change was 40 degrees Celsius, for example. Let's look at the data.\n",
    "```\n",
    "logger.info(df_no_nulls_limited.describe())\n",
    "logger.info(df_no_nulls_limited.loc[df_no_nulls_limited['battery_drain'] < 0])\n",
    "```\n",
    "\n",
    "We are definitely seeing one weird result: a negative battery drainage. That can't be. Let's remove the data point and continue:\n",
    "```\n",
    "df_no_nulls_limited_final = df_no_nulls_limited.copy().loc[\n",
    "        df_no_nulls_limited['battery_drain'] >= 0]\n",
    "logger.info(df_no_nulls_limited_final.describe()) \n",
    "\n",
    "df_no_nulls_limited_final.to_csv(\n",
    "    path_or_buf=filename_to_export,\n",
    "    index=False\n",
    "```\n",
    "\n",
    "Awesome! We have now processed the data and it's ready to be modeled. Let's run the job again, just to make sure that all of\n",
    "the scripts function as they should! Remember, just go back to setup.ipynb and re-run the \"!vdk run...\" command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55b51e",
   "metadata": {},
   "source": [
    "### Step 7: Data Job - Build the Model, Test the Model, Save the Model\n",
    "Ok, so let's create a copy of 30_process_data.py and name it 40_build_model.py. Open it up.\n",
    "\n",
    "Let's delete everything above the run function again and paste the following:\n",
    "```\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import pickle\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from vdk.api.job_input import IJobInput\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "os.chdir(pathlib.Path(__file__).parent.absolute())\n",
    "```\n",
    "\n",
    "Let's now turn our attention to the run function. Let's get rid of everything in there and begin building our code.\n",
    "First, we're going to want to set the file name to import, create a sub-folder to house our model, and read in the data.\n",
    "```\n",
    "logger.info('executing program: ' + os.path.basename(__file__))\n",
    "\n",
    "filename_to_import = 'VW ID. 3 Pro Max EV Consumption_Model_Data.csv'\n",
    "\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer=filename_to_import)\n",
    "```\n",
    "\n",
    "Let's now split the data into four chunks: x_train, y_train, x_test, and y_test. x denotes the independent variables or the\n",
    "predictor variables and y denotes the variable we are trying to estimate - i.e., battery drainage. What's the idea here?\n",
    "Well, suppose that you build a really good model. Well, how do you know it's good? By setting aside some testing data,\n",
    "you can create a model purely on the training data and THEN test that model on data it has not seen before. That way,\n",
    "you'll know how good the model is because you actually have the testing data's dependent variable (y test) and can measure it\n",
    "against your model's prediction. The code below shows one way to split the data, but there are many, and we encourage \n",
    "you to read up more about this. We will split the data based on a pre-defined random state, so that the numbers are\n",
    "reproducible. We will take 20 percent of the data and put it aside as testing data:\n",
    "```\n",
    "y = df.copy()[['battery_drain']]\n",
    "x = df.copy().drop('battery_drain', axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "   x,\n",
    "   y,\n",
    "   test_size=0.2,\n",
    "   random_state=22\n",
    " )\n",
    "```\n",
    "\n",
    "Let's check out what this looks like:\n",
    "```\n",
    "data_sets = {\n",
    "    \"x_train\": x_train,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test\n",
    "}\n",
    "for name, data_set in data_sets.items():\n",
    "    logger.info(f\"The shape of the {name} dataset is: {data_set.shape}\")\n",
    "```\n",
    "\n",
    "We will want to check that none of the independent variables (or predictors) are not heavily correlated with one another.\n",
    "We can do this through a correlation plot:\n",
    "```\n",
    "sns.set(\n",
    "    style='ticks',\n",
    "    rc={'figure.figsize': (18, 14)}\n",
    ")\n",
    "sns.heatmap(\n",
    "    x_train.corr(),\n",
    "    annot=True,\n",
    "    cmap=sns.diverging_palette(10, 250, n=240)\n",
    ")\n",
    "plt.savefig('explore_data/features_correlation.png')\n",
    "# plt.show() the following features need to be dropped: is_bridgestone_tyre, is_summer, and ac_use\n",
    "# plt.clf()\n",
    "```\n",
    "\n",
    "For those that appear to be heavily correlated with other main variables, we can drop them from both the train and test:\n",
    "```\n",
    "predictive_data_sets = [x_train, x_test]\n",
    "for data_set in predictive_data_sets:\n",
    "    data_set.drop(['is_bridgestone_tyre', 'is_summer', 'ac_use'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "It is often good practice to perform feature selection. That means to narrow down the features/predictors you want to use to\n",
    "be in your model. There are many ways to do this, but one is called Lasso regression. It penalizes the coefficients of the\n",
    "least important predictors in your model and brings them to 0. Thus, we can manually take them out. Please make sure to\n",
    "normalize the data first! We've left a little [BLANK] in there for you!\n",
    "```\n",
    "lasso = LassoCV(normalize=[BLANK], random_state=22)\n",
    "lasso.fit(\n",
    "    x_train,\n",
    "    y_train\n",
    ")\n",
    "lasso = pd.Series(lasso.coef_, index=x_train.columns)\n",
    "\n",
    "features_to_delete = list(lasso[lasso == 0].index)\n",
    "for data_set in predictive_data_sets:\n",
    "    data_set.drop(features_to_delete, axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "OK, let's finally fit our model to the training data! We can then create a prediction based on the predictors' values\n",
    "from x_test. Then, we can compare how those predictions compare against y_test. Neat, huh?!\n",
    "```\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x_train, y_train)\n",
    "\n",
    "y_pred = linreg.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['battery_drain_prediction'])\n",
    "actual_vs_predicted = pd.concat(\n",
    "    [y_test.copy().reset_index(drop=True), y_pred.copy().reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "actual_vs_predicted.to_csv('model/actual_vs_model_predicted_battery_drain_test.csv')\n",
    "```\n",
    "\n",
    "Now that we have the true values of battery drain from y_test and the predicted battery drain from y_pred, we can get\n",
    "some measures of model quality, like the mean squared error, mean absolute error, and r squared!\n",
    "```\n",
    "measurements = {\n",
    "    'mean squared error': mean_squared_error,\n",
    "    'mean absolute error': mean_absolute_error,\n",
    "    'R2': r2_score}\n",
    "for measure, func in measurements.items():\n",
    "    logger.info(f\"The {measure} is: {func(y_pred, y_test)}\")\n",
    "```\n",
    "\n",
    "Let's also extract the coefficients of the model - who knows! Maybe we will need them!\n",
    "```\n",
    "coeff = pd.DataFrame(linreg.coef_).transpose()\n",
    "inter = pd.DataFrame(linreg.intercept_).transpose()\n",
    "inter_and_coeff = pd.concat(\n",
    "    [inter, coeff],\n",
    "    ignore_index=True\n",
    ")\n",
    "inter_and_coeff.columns = ['coefficients']\n",
    "intercept = ['intercept']\n",
    "intercept.extend(x_train.columns.to_list())\n",
    "feature_names = pd.DataFrame(\n",
    "    intercept,\n",
    "    columns=['feature']\n",
    ")\n",
    "model_coeffs = pd.concat(\n",
    "    [inter_and_coeff, feature_names],\n",
    "    axis=1,\n",
    "    join='outer'\n",
    ")\n",
    "model_coeffs.to_csv('model/model_coefficients.csv')\n",
    "```\n",
    "\n",
    "Finally, let's save the model!\n",
    "```\n",
    "filename = 'model/amld_linear_regression_model.sav'\n",
    "pickle.dump(linreg, open(filename, 'wb'))\n",
    "```\n",
    "\n",
    "Let's run the data job again!\n",
    "If it all goes through, congratulations! You have built and ran your first data job. No better feeling!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a335e6",
   "metadata": {},
   "source": [
    "### Step 8: Let's Build a Streamlit Visualization!\n",
    "Now that we have finished with the data job, let's use that hard-earned model to make a cool dashboard!\n",
    "\n",
    "First, let's take a look at the coefficients from our model and let's build a parameters file that will serve as the\n",
    "user input generated values for our predictors. They will be fed to the model, and we'll have a prediction for whatever\n",
    "values a user gives us for the predictors! Neat!\n",
    "\n",
    "Open up the linear-reg-data-job sub-folder and find the model_coefficients .CSV file. Take a note of the feature\n",
    "column, as that will show you which predictors made it to the model and which predictors we will need to add in the\n",
    "parameters.py file. As such, let's go back to the main folder and create a new script. Title it 'parameters.py' and\n",
    "input the following code inside it:\n",
    "```\n",
    "parameters = {\n",
    "    'charge_level_start': dict(\n",
    "        label=\"Select Value for the Starting Charge Level\",\n",
    "        value=80,\n",
    "        max_value=100,\n",
    "        min_value=1\n",
    "    ),\n",
    "    'temperature_start_c': dict(\n",
    "        label=\"What's the Temperature Outside? In Celsius Please!\",\n",
    "        value=20,\n",
    "        max_value=50,\n",
    "        min_value=-50\n",
    "    ),\n",
    "    'distance_km': dict(\n",
    "        label=\"How Many Kilometers Are We Going to Drive?\",\n",
    "        value=50,\n",
    "        max_value=1000,\n",
    "        min_value=1\n",
    "    ),\n",
    "    'average_speed_kmh': dict(\n",
    "        label=\"What's the Average Speed We Expect (in Kilometers per Hour)?\",\n",
    "        value=60,\n",
    "        max_value=300,\n",
    "        min_value=1\n",
    "    ),\n",
    "    'average_consumption_kwhkm': dict(\n",
    "        label=\"Any Guesses on the Average Consumption (in kwhkm)?\",\n",
    "        value=15,\n",
    "        max_value=50,\n",
    "        min_value=1\n",
    "    ),\n",
    "    'heated_seats': dict(\n",
    "        label=\"Do We Plan on Using the Heated Seats? (1 = Yes, 0 = No)\",\n",
    "        value=1,\n",
    "        min_value=0,\n",
    "        max_value=1\n",
    "    ),\n",
    "    'eco_mode': dict(\n",
    "        label=\"Do We Plan on Using the Eco Mode? (1 = Yes, 0 = No)\",\n",
    "        value=1,\n",
    "        min_value=0,\n",
    "        max_value=1\n",
    "    )\n",
    "}\n",
    "```\n",
    "This will help set up the user input interface and will bound the user to a min and max value for the predictors.\n",
    "In other words, a user won't be able to say that they expect to drive a 10000 km per hour.\n",
    "\n",
    "Close out of the file and create a new Python script in the main folder. This will be the main script for the Streamlit\n",
    "visualization. Title it \"build_streamlit_dashboard.py\".\n",
    "\n",
    "Inside it, let's start with importing the libraries and adding titles:\n",
    "```\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pathlib\n",
    "import streamlit as st\n",
    "from parameters import parameters\n",
    "\n",
    "st.title('Electric Cars and Battery Drain Linear Regression Example')\n",
    "```\n",
    "\n",
    "Now, let's do the first section of our visualization: the model showcase and quality measure:\n",
    "```\n",
    "os.chdir(pathlib.Path(__file__).parent.absolute())\n",
    "actual_vs_pred_loc = 'linear-reg-data-job/model/actual_vs_model_predicted_battery_drain_test.csv'\n",
    "model_loc = 'linear-reg-data-job/model/amld_linear_regression_model.sav'\n",
    "```\n",
    "\n",
    "Now, we'll read in the actual versus predicted data set that we created in our data job and visualize it in the dashboard:\n",
    "```\n",
    "actual_vs_pred = pd.read_csv(actual_vs_pred_loc, usecols=range(1, 3))\n",
    "actual_vs_pred['absolute_difference'] = abs(\n",
    "    actual_vs_pred['battery_drain'] - actual_vs_pred['battery_drain_prediction']\n",
    ")\n",
    "st.dataframe(actual_vs_pred)\n",
    "battery_drain = actual_vs_pred[['battery_drain']]\n",
    "battery_drain_prediction = actual_vs_pred[['battery_drain_prediction']]\n",
    "```\n",
    "\n",
    "Let's now visualize the model's quality metrics!\n",
    "```\n",
    "mse = round(mean_squared_error(battery_drain, battery_drain_prediction), 2)\n",
    "mae = round(mean_absolute_error(battery_drain, battery_drain_prediction), 2)\n",
    "r2  = round(r2_score(battery_drain, battery_drain_prediction), 2)\n",
    "\n",
    "st.metric(\"The Mean Squared Error of This Model On This Testing Data Is:\", mse)\n",
    "st.metric(\"The Mean Absolute Error of This Model On This Testing Data Is:\", mae)\n",
    "st.metric(\"The R2 is:\", r2)\n",
    "```\n",
    "\n",
    "Now let's add the second section, which will allow the user to select the predictors' values that will feed into the\n",
    "model and give them an estimated battery drain!\n",
    "```\n",
    "st.header('How Much Will Your Electric Car Battery Drain? You May Be Surprised!')\n",
    "st.write(\"Enter Your Custom Values in the SideBar - Please Enter Sensible Values Only!\")\n",
    "\n",
    "\n",
    "results = {}\n",
    "for measurement, params in parameters.items():\n",
    "    output = st.sidebar.number_input(**params)\n",
    "    results[measurement] = output\n",
    "results_df = pd.DataFrame(results, index=[0])\n",
    "\n",
    "\n",
    "pickled_model = pickle.load(open(model_loc, 'rb'))\n",
    "\n",
    "\n",
    "estimate = pickled_model.predict(results_df)\n",
    "```\n",
    "\n",
    "Let's fix some minor stuff like: if the user enters inputs that generate a drain prediction higher than the initial charge,\n",
    "let's tell them that they will run out of battery! Or if the model somehow predicts a negative charge, to fix that to 0.\n",
    "I know, an easy way out! :)\n",
    "```\n",
    "if estimate > results['charge_level_start']:\n",
    "    estimate = results['charge_level_start']\n",
    "    st.metric(\"Your Estimated Battery Drainage (in Percent) Is:\", estimate)\n",
    "    st.write(\"Note: The Model's Estimate Exceeds the Starting Level Charge; Thus Estimate is Capped\")\n",
    "elif estimate < 0:\n",
    "    estimate = 0\n",
    "    st.metric(\"Your Estimated Battery Drainage (in Percent) Is:\", estimate)\n",
    "else:\n",
    "    st.metric(\"Your Estimated Battery Drainage (in Percent) Is:\", estimate)\n",
    "```\n",
    "\n",
    "Congratulations! You have built your first Streamlit dashboard that even allows for a user to enter inputs! How cool is that?\n",
    "\n",
    "As a last step, go back to the setup.ipynb and type the following code:\n",
    "```\n",
    "!streamlit run building_streamlit_dashboard.py\n",
    "```\n",
    "\n",
    "You will get an output, but the kernel will be stuck. That's okay! Just open a new tab in your browser,\n",
    "copy the link of the MyBinder environment, delete everything after \"user/blah blah blah\" and paste \"/proxy/8501/\"\n",
    "So, something like this: \n",
    "```\n",
    "https://hub.gke2.mybinder.org/user/alexanderavramo-n-example-empty-zkd8q00p/proxy/8501/\n",
    "```\n",
    "\n",
    "The Streamlit dashboard will now show up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ba64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
